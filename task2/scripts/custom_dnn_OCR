#!/usr/bin/python3

import sys
import math
import rospy
import cv2
import numpy as np
from numpy.linalg import norm
# import sensor_msgs.point_cloud2 as pc2
# from sklearn.decomposition import PCA
# from sklearn.cluster import DBSCAN
import tf2_geometry_msgs
import tf2_ros
from tf import transformations

# import pygame
from os.path import dirname, join
from matplotlib import use
import matplotlib.pyplot as plt
from sensor_msgs.msg import Image
from geometry_msgs.msg import PointStamped, Vector3, Pose, PoseWithCovarianceStamped, PoseStamped, Pose, Quaternion
from cv_bridge import CvBridge, CvBridgeError
from visualization_msgs.msg import Marker, MarkerArray
from std_msgs.msg import ColorRGBA
from std_msgs.msg import String
from task2.srv import movementRequest

import dlib

from sound_play.libsoundplay import SoundClient

from std_msgs.msg import Int8, Float32MultiArray


import pytesseract
print(pytesseract.__version__) # prints this 

from fuzzywuzzy import fuzz
from collections import Counter
from std_msgs.msg import Float32MultiArray

import socket

use('GTK3Agg')


def cosine_distance(v1 : np.ndarray, v2: np.ndarray) -> float:
    return 1 - np.dot(v1, v2) / (norm(v1) * norm(v2))


def euclidean_distance(x1, y1, x2, y2) -> float:
    return np.sqrt((x1-x2)**2 + (y1-y2)**2)


class face_localizer:
    def __init__(self):
        # init rospy node called face_localizer
        rospy.init_node('face_localizer', anonymous=True)

        #counter for posters
        self. poster_counter = 0 

        # I'm sorry. I'm so sorry. I'm so, so sorry.

        self.rgb_image_message = None
        self.depth_image_message = None

        self.rgb_image = None
        self.depth_image = None

        self.custom_config = r'--psm 12' # 12 kinda works

        # An object we use for converting images between ROS format and OpenCV format
        self.bridge = CvBridge()
        yellow={176, 240, 252}

        # The function for performin HOG face detection
        #self.face_detector = dlib.get_frontal_face_detector()
        protoPath = join(dirname(__file__), "deploy.prototxt.txt")
        modelPath = join(dirname(__file__), "res10_300x300_ssd_iter_140000.caffemodel")
        self.face_rec_model = dlib.face_recognition_model_v1(join(dirname(__file__), 'dlib_face_recognition_resnet_model_v1.dat'))

        self.face_net = cv2.dnn.readNetFromCaffe(protoPath, modelPath)

        # Initialize OCR using pytesseract [ NEW ] =========================================================
        # self.face_net = cv2.dnn.readNetFromCaffe(protoPath, modelPath)
        # self.ocr = pytesseract.Tesseract()
        pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'
        self.ocr = pytesseract

        # A help variable for holding the dimensions of the image
        self.dims = (0, 0, 0) # width, height, number of channels 

        # Marker array object used for showing markers in Rviz
        self.marker_array = MarkerArray()
        self.marker_num = 1
        self.desired_no_of_posters = 3
        self.desired_no_of_detections = 7 # previously 3, task2 doesn't state number of faces.

        # Subscribe to the image and/or depth topic
        # self.image_sub = rospy.Subscriber("/camera/rgb/image_raw", Image, self.image_callback)
        # self.depth_sub = rospy.Subscriber("/camera/depth/image_raw", Image, self.depth_callback)


        # current robot location
        self.x = 0
        self.y = 0
        self.z_ang = 0
        self.w_ang = 0

        self.target_dict = {} # 1000 ->  (color_of_ring, [vector_representation_of_face])

        #Publisher for speech.py	
        self.speech_pub = rospy.Publisher('/start_dialog', String, queue_size=10) 

        # prison position
        self.prison_ring_pb = rospy.Publisher('/prison_ring', String, queue_size = 5)

        # face embedding publisher
        self.top_bounty_encoded_pub = rospy.Publisher('/top_bounty_encoded', Float32MultiArray, queue_size = 5)

        # Publiser for the visualization markers
        self.markers_pub = rospy.Publisher('all_faces', MarkerArray, queue_size=1000)
        
        #publisher to check if all faces(people) have been found
        self.face_status_pub = rospy.Publisher('/face_status', Int8, queue_size=5)

        #publisher to check if all posters have been found
        self.poster_and_cylinder_status_pub = rospy.Publisher('/poster_and_cylinder_status', Int8, queue_size=5) # !!!!!!!!!
       
        #self.face_and_cylinder_status_pub = rospy.Publisher('/face_and_cylinder_status', Int8, queue_size=5) # !!!!!!!!!
        # Publishers for movement and robot pose location
        self.approach_pub = rospy.Publisher('/move_base_simple/goal', PoseStamped, queue_size=20) # LEGACY?
        self.curr_location = rospy.Subscriber("/amcl_pose", PoseWithCovarianceStamped, self.curr_location_callback)
        self.movement_client = rospy.ServiceProxy('/request_movement', movementRequest) # LEGACY?

        # self.sound_pub = rospy.Publisher("/sound_play/goal", SoundRequest, queue_size=10)

        # Object we use for transforming between coordinate frames
        self.tf_buf = tf2_ros.Buffer()
        self.tf_listener = tf2_ros.TransformListener(self.tf_buf)

        self.sound_handle = SoundClient()
        # rospy.sleep(1)
        self.sound = self.sound_handle.waveSound("/home/manga/catkin_workspace/src/RINS-task1/RINS-main/StickEmUp.wav", volume=1)

        self.inital_ = True
        self.global_pose_ = PoseStamped()
        self.sign = 0


    def calculate_quaternion(self, marker_x, marker_y, robot_x, robot_y):
        # Calculate the angle between robot and marker
        angle = math.atan2(marker_y - robot_y, marker_x - robot_x)
        
        # Create quaternion for yaw rotation
        quaternion = transformations.quaternion_from_euler(0, 0, angle)
        
        # Normalize the quaternion
        normalized_quaternion = transformations.unit_vector(quaternion)
        
        # Convert to geometry_msgs.Quaternion
        result = Quaternion()
        result.x = normalized_quaternion[0]
        result.y = normalized_quaternion[1]
        result.z = normalized_quaternion[2]
        result.w = normalized_quaternion[3]
        
        return result


    def obtain_newest_information(self):
        try:
            self.rgb_image_message = rospy.wait_for_message("/camera/rgb/image_raw", Image)
        except Exception as e:
            print(e)
            return 0

        try:
            self.depth_image_message = rospy.wait_for_message("/camera/depth/image_raw", Image)
            # print(depth_image_message.encoding)
        except Exception as e:
            print(e)
            return 0
        
        # try:
        #     cloud_msg = rospy.wait_for_message("/camera/depth/points", pc2.PointCloud2)
        #     # print(cloud_msg.encoding)
        # except Exception as e:
        #     print(e)
        #     return 0

        # Convert the images into a OpenCV (numpy) format

        try:
            self.rgb_image = self.bridge.imgmsg_to_cv2(self.rgb_image_message, "bgr8")
        except CvBridgeError as e:
            print(e)

        try:
            self.depth_image = self.bridge.imgmsg_to_cv2(self.depth_image_message, "32FC1")
        except CvBridgeError as e:
            print(e)



    def curr_location_callback(self, msg: PoseWithCovarianceStamped):
        self.x = msg.pose.pose.position.x
        self.y = msg.pose.pose.position.y
        self.z_ang = msg.pose.pose.orientation.z
        self.w_ang = msg.pose.pose.orientation.w



    def get_pose(self,coords,dist,stamp):
        # Calculate the position of the detected face in the coordinate system
        ## Above line lies, it actually returns relative position to the robot.
        ## to obtain the absolute position, we need to transform relative position by adding robot position.

        k_f = 554 # kinect focal length in pixels

        x1, x2, y1, y2 = coords

        face_x = self.dims[1] / 2 - (x1+x2)/2.
        face_y = self.dims[0] / 2 - (y1+y2)/2.

        angle_to_target = np.arctan2(face_x,k_f)

        # Get the angles in the base_link relative coordinate system
        x, y = dist*np.cos(angle_to_target), dist*np.sin(angle_to_target)

        ### Define a stamped message for transformation - directly in "base_link"
        #point_s = PointStamped()
        #point_s.point.x = x
        #point_s.point.y = y
        #point_s.point.z = 0.3
        #point_s.header.frame_id = "base_link"
        #point_s.header.stamp = rospy.Time(0)

        # Define a stamped message for transformation - in the "camera rgb frame"
        point_s = PointStamped()
        point_s.point.x = -y
        point_s.point.y = 0
        point_s.point.z = x
        point_s.header.frame_id = "camera_rgb_optical_frame"
        point_s.header.stamp = stamp


        # Get the point in the "map" coordinate system
        try:
            point_world = self.tf_buf.transform(point_s, "map")

            # Create a Pose object with the same position
            pose = Pose()
            pose.position.x = point_world.point.x
            pose.position.y = point_world.point.y
            pose.position.z = point_world.point.z
        except Exception as e:
            print(e)
            pose = None

        return pose
    

    def overwrite_location(self):
        """
        Overwrite the current location with the global pose from get_robot_pose.
            * Placeholder to see if this works better than /amcl_pose
        """
        self.x = self.global_pose_.translation.x
        self.y = self.global_pose_.translation.y
        self.z_ang = self.sign * (self.global_pose_.rotation.z)
        self.w_ang = self.sign * (self.global_pose_.rotation.w)
        print("Inside Overwrite: x: " + str(self.x) + " y: " + str(self.y) + " z_ang: " + str(self.z_ang) + " w_ang: " + str(self.w_ang))


    def get_robot_pose(self):
        """
        Get the pose of the robot in the global frame.
        Set the global pose to represent the pose of the robot in the global frame.
        Format of the global pose is PoseStamped:
        global_pose.translation is our position in the global frame.
        global_pose.rotation is our orientation in the global frame in quarterion format.
        """
        transformed= self.tf_buf.lookup_transform("map", "base_link", rospy.Time(0), rospy.Duration(10.0))
        # print(transformed)
        self.global_pose_ = transformed.transform
        if self.inital_:
            self.inital_ = False
            ## here we have a situation where in amcl rotation w cannot be negative
            ## but in our transformation it can. If W is negative, we know we  have to flip the sign
            ## this is because of Q representing a rotation matrix, and -Q representing the absolute same rotation
            ## quarterion representation of the angle: [x, y, z, w]
            if self.global_pose_.rotation.w >= 0:
                self.sign = 1
            else:
                self.sign = -1
        # print(self.global_pose_)
        self.overwrite_location()

    
    
    def check_if_tagged(self, new_marker):
        for idx, marker in enumerate(self.marker_array.markers):
        # Ugly disgusting assignments, so it's a bit more readable down the line
            x_new = new_marker.pose.position.x
            y_new = new_marker.pose.position.y
            x_old = marker.pose.position.x
            y_old = marker.pose.position.y

            distance = euclidean_distance(x_new, y_new, x_old, y_old)  # calc distance between possible markers
            print(f"Distance between new and marker with idx {idx}: {distance}")  # sanity check print

            if (distance < 0.50):   # marker array of detected faces
                new_marker.id = marker.id
                marker = new_marker
                return True

        # assume new marker:
        #self.marker_array.markers.append(new_marker)
        print("Face detected is not tagged yet, adding new marker")
        return False

        # If the new marker is close to an old marker, we don't add it, as it is probably a duplicate

    def find_faces(self):
        # print('I got a new image!')
        self.get_robot_pose()

        self.obtain_newest_information()

        # Set the dimensions of the image
        self.dims = self.rgb_image.shape
        h = self.dims[0] 
        w = self.dims[1]

        # Tranform image to gayscale
        #gray = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2GRAY)

        # Do histogram equalization
        #img = cv2.equalizeHist(gray)

        # Detect the faces in the image
        #face_rectangles = self.face_detector(rgb_image, 0)
        blob = cv2.dnn.blobFromImage(cv2.resize(self.rgb_image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))
        self.face_net.setInput(blob)
        face_detections = self.face_net.forward()

        poster = False

        for i in range(0, face_detections.shape[2]):
            confidence = face_detections[0, 0, i, 2]
            # print("current face_confidence = ", confidence)

            if confidence >= 0.60: # 0.80, 0.75, 0.65 still not consistent enough trying to get longer poster detection

                x_snap = self.x
                y_snap = self.y
                w_snap = self.w_ang
                z_snap = self.z_ang


                box = face_detections[0,0,i,3:7] * np.array([w,h,w,h])
                box = box.astype('int')
                x1, y1, x2, y2 = box[0], box[1] , box[2], box[3]

               # greedy capture more area, bit whoops but sure
                padding_factor = 13

                # Find the distance to the detected face
                # face_distance = float(np.nanmean(depth_image[y1:y2,x1:x2]))
                if np.isnan(np.nanmean(self.depth_image[y1:y2, x1:x2])):
                    print("Invalid depth values in the face region.")
                    return
                
                if np.isnan(np.nanmean(self.rgb_image[y1:y2, x1:x2])):
                    print("Invalid rgb values in the face region.")
                    return

                # Calculate face_distance
                face_distance = float(np.nanmean(self.depth_image[y1:y2, x1:x2]))

                print('Distance to face', face_distance)

                if np.isnan(face_distance):
                    print("Invalid or missing depth value. Skipping calculation.")
                    return

                else:
                    x1 -= int(padding_factor / (face_distance))
                    x2 += int(padding_factor / (face_distance))
                    # y1 -= 12
                    # y2 += 15

                # Extract region containing face
                face_region = self.rgb_image[y1:y2, x1:x2] # start and ending coords on x and y axis

                if face_region.size > 0:
                    topleft_px = np.uint8(face_region[0, 0])
                    topright_px = np.uint8(face_region[0, -1])
                    botleft_px = np.uint8(face_region[-1, 0])
                    botright_px = np.uint8(face_region[-1, -1])
                else:
                    print("No valid face region found.")
                    return
                    # Handle the case when face_region is empty or has incorrect dimensions


                # Get the dimensions of the face_region image
                # yellow_array = np.array([176, 240, 252], dtype = np.uint8)
                yellow_array = np.array([205, 255, 255], dtype = np.uint8) # anze
                height, width, _ = face_region.shape

                # left_px = np.uint8(face_region[height//2, 0])
                # right_px = np.uint8(face_region[height//2, -1])

                # topleft_px = np.uint8(face_region[0, 0])
                # topright_px = np.uint8(face_region[0, -1])
                # botleft_px = np.uint8(face_region[-1, 0])
                # botright_px = np.uint8(face_region[-1, -1])

                # yellow_color = np.array([176, 240, 252], dtype=np.uint8)
                # Convert single pixels to 3-channel images
                meaned_yellow_l = np.mean([topleft_px, botleft_px], axis=0)
                meaned_yellow_r = np.mean([topright_px, botright_px], axis=0)
                print("meaned yellow LEFT:", meaned_yellow_l)
                print("meaned yellow RIGHT:", meaned_yellow_r)

                # Convert the yellow color from BGR to RGB
                rgb_yellow = yellow_array
                # print(yellow_array)
                # print("rgb_yell", rgb_yellow)
                # Convert the meaned yellow color from BGR to RGB
                rgb_meaned_l = meaned_yellow_l
                rgb_meaned_r = meaned_yellow_r
                print("rgb_mean LEFT", rgb_meaned_l)
                print("rgb_mean RIGHT", rgb_meaned_r)

                color_distance_euclidean_l = np.linalg.norm(rgb_yellow - rgb_meaned_l)
                print("==================== meaned left", color_distance_euclidean_l)

                color_distance_euclidean_r = np.linalg.norm(rgb_yellow - rgb_meaned_r)
                print("==================== meaned right", color_distance_euclidean_r)

                color_threshold = 25.0

                if color_distance_euclidean_l <= color_threshold or color_distance_euclidean_r <= color_threshold:
                    print("The overall color is similar to yellow.")
                    poster = True

                # Get the time that the depth image was recieved
                depth_time = self.depth_image_message.header.stamp

                # Find the location of the detected face
                pose = self.get_pose((x1,x2,y1,y2), face_distance, depth_time)


                if pose is not None:

                    # Create a marker used for visualization
                    # using relative coordinates of pose
                    # num = self.marker_num + 1
                    marker = Marker()
                    marker.header.stamp = rospy.Time(0)
                    marker.header.frame_id = 'map'
                    marker.pose = pose
                    marker.type = Marker.CUBE
                    marker.action = Marker.ADD
                    marker.frame_locked = False
                    marker.lifetime = rospy.Duration.from_sec(10)
                    
                    marker.scale = Vector3(0.1, 0.1, 0.1)
                    if(poster == True):
                        marker.color = ColorRGBA(1, 0 ,0 ,1)
                    else:
                        marker.color = ColorRGBA(0, 1, 0, 1)
                    
                    if (face_distance < 1.9): # 1.3
                        already_detected = self.check_if_tagged(marker)
                        print("Already detected result: ",  already_detected)
                        if already_detected:
                            print('Face already marked!')
                        else:
                            
                            # if the face hasn't been marked yet, we marked it inside of already_detected
                            # so we can just show the face.
                            face_region = self.rgb_image[y1:y2, x1:x2]
                            face_region = cv2.cvtColor(face_region, cv2.COLOR_BGR2RGB)
                            ## old snap position
                            # self.get_robot_pose()

                            # x_snap = self.x
                            # y_snap = self.y
                            # w_snap = self.w_ang
                            # z_snap = self.z_ang

                            # new_goal = self.create_approach_goal(marker, x_snap, y_snap, z_snap, w_snap)
                            new_goal = self.create_movement_request(marker, x_snap, y_snap, z_snap, w_snap)


                            print("New face detected: Initiating approach to face")

                            if(poster == True and self.poster_counter<=2):
                                
                                self.marker_num += 1
                                marker.id =  self.marker_num + 1
                                # self.approach_pub.publish(new_goal)

                                rospy.wait_for_service('/request_movement')
                                # req = movementRequest(new_goal)
                                # req._request_class.goal = new_goal
                                # req.goal = new_goal
                                try:
                                    response = self.movement_client(new_goal)
                                except rospy.ServiceException as e:
                                    print("Service call failed: %s"%e)
                                    return
                                
                                if(response.result == 1):
                                    print("Movement request successful.")
                                    self.get_robot_pose()
                                elif(response.result == 0):
                                    print("Movement request failed.")
                                    return
                                
                                ##
                                ## Here we fix orientation
                                ## 
                                x_snap = self.x
                                y_snap = self.y
                                res_quart = self.calculate_quaternion(marker.pose.position.x, marker.pose.position.y, x_snap, y_snap)
                                w_snap = res_quart.w
                                z_snap = res_quart.z
                                new_goal = self.create_movement_request(marker, x_snap, y_snap, z_snap, w_snap)


                                try:
                                    response = self.movement_client(new_goal)
                                except rospy.ServiceException as e:
                                    print("Service call failed: %s"%e)
                                    return
                                
                                if(response.result == 1):
                                    print("Movement request successful.")
                                    self.get_robot_pose()
                                elif(response.result == 0):
                                    print("Movement request failed.")
                                    return


                                self.poster_counter+=1
                                self.obtain_newest_information()

                                # Set the dimensions of the image
                                self.dims = self.rgb_image.shape
                                h = self.dims[0] 
                                w = self.dims[1]

                                # Detect the faces in the image
                                #face_rectangles = self.face_detector(rgb_image, 0)
                                blob = cv2.dnn.blobFromImage(cv2.resize(self.rgb_image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))
                                self.face_net.setInput(blob)
                                face_detections = self.face_net.forward()

                                box = face_detections[0,0,0,3:7] * np.array([w,h,w,h])
                                box = box.astype('int')
                                x1, y1, x2, y2 = box[0], box[1] , box[2], box[3]

                                face_encodings = []
                                face_landmarks = dlib.shape_predictor(join(dirname(__file__), 'shape_predictor_68_face_landmarks.dat'))(self.rgb_image, dlib.rectangle(x1, y1, x2, y2))
                                face_encoding = np.array(self.face_rec_model.compute_face_descriptor(self.rgb_image, face_landmarks))
                                face_encodings.append(face_encoding)

                                print(face_encodings)

                                face_distance = float(np.nanmean(self.depth_image[y1:y2, x1:x2]))
                                padding_factor = 13

                                print('Distance to face', face_distance)

                                if not np.isnan(face_distance):
                                    x1 -= int(padding_factor / face_distance)
                                    x2 += int(padding_factor / (face_distance))
                                else :
                                    print("face_distance ERROR.")
                                    return

                                # rospy.sleep(1000)

                                # Specify the desired width and height
                                # new_width = 200
                                # new_height = 200

                                # # Resize the face_region image
                                # face_region = cv2.resize(rgb_image, (new_width, new_height))
                                why_padding1 = 65
                                why_padding2 = 50 # fresh out of my ass
                                x1 -= 30
                                x2 += 30
                                y1 += int(why_padding1 / face_distance)
                                y2 += int(why_padding2 / face_distance)

                                face_region = self.rgb_image[y1:y2, x1:x2] 
                                if face_region.size > 0:
                                    plt.imshow(face_region)
                                    plt.axis('off')
                                    plt.show()
                                else:
                                    print("can't show img.")
                                    self.obtain_newest_information()
                                    blob = cv2.dnn.blobFromImage(cv2.resize(self.rgb_image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))
                                    self.face_net.setInput(blob)
                                    face_detections = self.face_net.forward()

                                    box = face_detections[0,0,0,3:7] * np.array([w,h,w,h])
                                    box = box.astype('int')
                                    x1, y1, x2, y2 = box[0], box[1] , box[2], box[3]

                                    face_distance = float(np.nanmean(self.depth_image[y1:y2, x1:x2]))
                                    padding_factor = 13

                                    print('Distance to face', face_distance)

                                    if not np.isnan(face_distance):
                                        x1 -= int(padding_factor / face_distance)
                                        x2 += int(padding_factor / (face_distance))
                                    else :
                                        print("face_distance ERROR.")
                                        return
                                    why_padding1 = 65
                                    why_padding2 = 50 # fresh out of my ass
                                    x1 -= 30
                                    x2 += 30
                                    y1 += int(why_padding1 / face_distance)
                                    y2 += int(why_padding2 / face_distance)


                                alpha = 3 # Contrast control
                                beta = -150 # Brightness control

                                # call convertScaleAbs function
                                adjusted = cv2.convertScaleAbs(face_region, alpha=alpha, beta=beta)

                                # Adjusted image data
                                #adjusted = self.adjust_image(image)

                                color_done =  False
                                prices = []  # empty array to store prices

                                if adjusted is not None: 
                                    #plt.imshow(face_region, cmap = "bgr8")
                                    
                                    #plt.axis('off')                        
                                    #plt.show()

                                    for i in range(80, 215, 5): # treshing time
                                        print("Processing poster ...")

                                        # Tranform image to gayscale
                                        # grey = cv2.cvtColor(adjusted, cv2.COLOR_BGR2GRAY)
                                        grey = cv2.cvtColor(face_region, cv2.COLOR_BGR2GRAY) # simplier perhaps :ooo
                                        grey = grey.astype(np.uint8)

                                        ret, binary_img = cv2.threshold(grey, i, 255, cv2.THRESH_BINARY)

                                        
                                        # plt.imshow(binary_img, cmap = "binary")
                                        # plt.imshow(binary_img, cmap='gray')
                                        # #plt.title("ADAPTIVE THRESHOLD image")
                                        # plt.axis('off')
                                        # plt.suptitle("ADAPTIVE THRESHOLD image - " + str(i) , y=0.95, fontsize=14)
                                        # plt.show()

                                        #Perform OCR on the face image
                                        try:
                                            text = self.ocr.image_to_string(binary_img, lang='eng', config=self.custom_config, timeout=3)
                                            # print(text)
                                        except Exception as e:
                                            print(f"OCR error: {e}")
                                            text = ""

                                        # EDIT TEXT =========================================================================
                                        text_array = text.split()
                                        combined_array = []
                                        temp_number = ""

                                        for word in text_array:
                                            if word.isdigit():
                                                temp_number += word
                                            elif temp_number:
                                                combined_array.append(temp_number)
                                                temp_number = ""
                                            combined_array.append(word) # 5 elements usually

                                        
                                        # GET BOUNTY ========================================================================
                                        digits_only = [word for word in combined_array if word.isdigit()]

                                        # print(combined_array)
                                        # print(text)
                                        # print(digits_only) # the digits 
                                        self.marker_array.markers.append(marker)

                                        for i in range(len(combined_array)):
                                            combined_array[i] = combined_array[i].replace(".", "").replace(",", "").replace("O", "0") # remove '.' and ',' and replace O with 0 
                                        
                                        if len(digits_only) >= 2:

                                            if digits_only[0] != digits_only[1]: # if not empty and not same 
                                                price = int(digits_only[0])
                                                prices.append(price * 1000)
                                                #print("BOUNTY:", price * 1000)

                                            if digits_only[0] == digits_only[1]:
                                                price = int(digits_only[0])
                                                prices.append(price)
                                                #print("BOUNTY:", price)

                                            # else :
                                            #     print("First 2 digits are not the same !")

                                        # else:
                                        #     print("Not enough elements in the digits_only list")


                                        # CHECK COLORS ==============================================================================
                                        if(color_done ==  False):
                                            colors_to_check = ["red", "green", "blue", "black"]
                                            
                                            threshold = 80  # Adjust the threshold value based on your requirements
                                            found_color = None

                                            for word in text_array:
                                                for color in colors_to_check:
                                                    similarity = fuzz.ratio(word.lower(), color.lower())
                                                    if similarity >= threshold:
                                                        found_color = color
                                                        break
                                                if found_color:
                                                    break

                                            if found_color:
                                                #print("Found color:", found_color)
                                                color_done = True
                                            # else:
                                            #     print("No matching color found.")
                                        
                                        #print("======================================================================")

                                else :
                                    print("No face region found.")


                                print("======================================================================")
                                print("FINAL COLOR: ", found_color)

                                # print("PRICES: ", prices)
                                # frequency of each price
                                price_counts = Counter(prices)

                                # most frequent price
                                final_price = price_counts.most_common(1)[0][0]

                                # if final_price % 1000 != 0:
                                #     final_price = (final_price // 1000) * 1000

                                print("FINAL PRICE: ", final_price)
                                print("======================================================================")
                                self.target_dict[final_price] = [found_color, face_encoding]


                            elif (poster == False and self.poster_counter > 2) :
                                if(self.marker_num == 4):
                                    # self.target_dict = {k: self.target_dict[k] for k in sorted(self.target_dict, key=lambda x: -int(x))}
                                    for i in range(0, 5):
                                        # print("unsorted buggery", self.target_dict)
                                        print(f"TOP BOUNTY: {max(self.target_dict)} --> {self.target_dict[max(self.target_dict)]}")

                                        ring_color = String()
                                        ring_color.data = self.target_dict[max(self.target_dict)][0]
                                        self.prison_ring_pb.publish(ring_color)
                                        print(f"publishing prison ring as: {ring_color}")
                                        
                                        embedding = Float32MultiArray()
                                        embedding.data = self.target_dict[max(self.target_dict)][1]
                                        self.top_bounty_encoded_pub.publish(embedding)
                                        print(f"publishing embedding vector as: {embedding}")

                                        

                                self.marker_num += 1
                                marker.id =  self.marker_num + 1
                                # req = movementRequest()
                                # req.goal = new_goal

                                try:
                                    response = self.movement_client(new_goal)
                                except rospy.ServiceException as e:
                                    print("Service call failed: %s"%e)
                                    return
                                
                                print("Is not poster !")
                                self.marker_array.markers.append(marker)
                                #self.sound.play()
                                #self.start_sub = rospy.Subscriber('/start_dialog', String, self.talk)
                                message = String()
                                message.data = "Start investigation!"
                                self.speech_pub.publish(message)
                                plt.imshow(face_region)
                                plt.axis('off')
                                plt.show()
                                ## need current robot position
                                
                        if(self.marker_num == 1 + self.desired_no_of_posters):
                            print("all posters found")
                            self.poster_and_cylinder_status_pub.publish(1)
                            
                        if self.marker_num == 1 + self.desired_no_of_detections: # because of course we start counting at 1, what is this... R? What is life ?
                            print("all posters and faces found")
                            self.face_status_pub.publish(1) # if all found
                            
        self.markers_pub.publish(self.marker_array)


    def create_approach_goal(self, marker, x_snap, y_snap, z_snap,  w_snap):
        new_goal = PoseStamped()
        new_goal.header.stamp = rospy.Time.now()
        new_goal.header.frame_id = 'map'
        new_goal.pose.position.x = (x_snap + marker.pose.position.x) * 0.50
        new_goal.pose.position.y = (y_snap + marker.pose.position.y) * 0.50
        new_goal.pose.orientation.z = z_snap
        new_goal.pose.orientation.w = w_snap

        return new_goal
    

    def create_movement_request(self, marker, x_snap, y_snap, z_snap,  w_snap, mul=0.50):
        new_goal = Pose()
        # new_goal.pose.position.x = (x_snap + marker.pose.position.x) * 0.50
        new_goal.position.x = (x_snap + marker.pose.position.x) * mul
        # new_goal.pose.position.y = (y_snap + marker.pose.position.y) * 0.50
        new_goal.position.y = (y_snap + marker.pose.position.y) * mul
        new_goal.orientation.z = z_snap
        new_goal.orientation.w = w_snap

        return new_goal


    def depth_callback(self,data): 

        try:
            depth_image = self.bridge.imgmsg_to_cv2(data, "32FC1")
        except CvBridgeError as e:
            print(e)

        # Do the necessairy conversion so we can visuzalize it in OpenCV
        
        image_1 = depth_image / np.nanmax(depth_image)
        image_1 = image_1*255
        
        image_viz = np.array(image_1, dtype=np.uint8)
        

def main():
        
        # pygame.init()

        # if not is_connected():
        #     sys.exit("No Internet connection available")

        face_finder = face_localizer()

        rate = rospy.Rate(1)
        while not rospy.is_shutdown():
            face_finder.find_faces()
            rate.sleep()

        cv2.destroyAllWindows()


if __name__ == '__main__':
    main()